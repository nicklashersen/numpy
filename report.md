# Report for assignment 3


## Project


Name: numpy


URL: https://github.com/nicklashersen/numpy/tree/doc


NumPy is the fundamental package needed for scientific computing with Python.


## Onboarding experience
        
Overall it did not take too long to get started with the project. However, we feel like the onboarding documentation could be improved since it is not clear from the README where to begin.


In order to build and run the project for development purposes; Python version 3 together with Cython and pytest where required. These requirements may not be regarded as additional tools so any documentation for these should not be needed. Although it is briefly described how to install these.


The build script did not install any other components other than the manually installed requirements.


Once the requirements had been installed the project built without any errors.


Using the runtests.py script different examples and tests could easily be run. Usage of this script is even documented within the file it self. Running all tests takes approximately 1-2 minutes on our systems.




## Complexity


The following table represents ten complex functions generated by Lizard:
  



The cyclomatic complexity of a function can be calculated through two different methods:


1. \pi - s + 2, where \pi is the number of decisions made. s is the number of terminating statements.
2. Drawing the control flow graph and calculating CNN = E - N + 2 * P, where E is the number of edges in the graph. N is the number of nodes in the graph. P is the number of connected components in the graph.

When calculating the CCN for five different functions by hand the results differed a little from the Lizard ones. We used the first method mentioned whereas Lizard probably uses another method, hence the differ in the result. An example for this might be that the function supports many different file path formats (absolute- and relative file name, etc.) which results in a lot of branches. The most of the functions we calculated are moderately long but is mostly made up of if-statements. The purpose of the functions we checked are add data files to configuration_data_files, add files under `data_path` to the list of `data_files` to be installed (and distributed), setting build settings etc. The documentation for the is not fully complete but is sufficient. 


## Coverage tool 


As a tool we only used the coverage tool which worked for some of us and not for others. It was easy to use, but it was a drag reading the output. 


The documentation was enough to use the tool but could have been better.


### DYI
Our coverage tool checks the branch coverage by giving each branch a unique ID. When that branch is taken when a test is executed the branch ID is written to a file called testcoverage.txt. Calls to secoverage.write_coverage(branch_id) are used to perform the write to testcoverage.txt. The tool is embedded into the existing runtests.py script and is executed when the tests are executed.


What is the quality of your own coverage measurement? Does it take into account ternary operators (condition ? yes : no) and exceptions, if available in your language? Since our tool requires manual instrumentation of the source code to add the calls to secoverage.write_coverage(branch_id) it is up to the one inserting the calls to decide how many and what operations to support. Currently loops, if-statements and exceptions are supported. Ternary operators are rewritten to if-statements for the coverage to work. That way the quality of the coverage measurement depends on how many and where calls to secoverage.write_coverage(branch_id) are inserted. Since they are inserted at every branch-statement that we could find the quality should be quite good. However, some operators like assert are not supported.


What are the limitations of your tool? How would the instrumentation change if you modify the code? A limitation of the tool is that is does not display branch coverage as a percentage but instead as the IDs of the branches taken. In order to obtain a percentage of the branch coverage the IDs in testcoverage.txt would have to be counted and compared to the total number of IDs for each function in coveragespec.txt. Another limitation is that assertions aren't accounted for by the tool. If the implementation of the code would change the calls to secoverage.write_coverage(branch_id) would have to modified so that there is one for each branch taken. Then branch IDs might also need to be updated.


If you can use an automated tool (as well), are your results consistent with the ones produced by existing tool(s)? Our results are more or less consistent with the results generated by codecov which is used by numpy as a coverage tool. Sometimes our tool reports that a function or branch has been tested while codecov reports that it does not and sometimes codecov reports a branch as tested while our tool does not recognize it.


### Evaluation


Report of old coverage: [link]


Report of new coverage: [link]


Test cases added:

```
test_setup_type_error
test_get_paths_env_var_is_file
test_get_paths_env_var_is_not_file
test_CCompiler_customize_need_cxx_true_gcc
test_CCompiler_customize_need_cxx_true_gcc_no_cxx
test_CCompiler_customize_need_cxx_true_non_gcc
test_get_atlas_version
test_swig_opts
test_f2py_opts
test_command_sequence
test_no_locale
test_long_double_representation_value_error
test_long_double_representation_intel_extended_12b
test_data_dir_raises
test_data_files_raises
test_setup_dist_help
test_setup_ext_library_string
```

## Refactoring


Functions refactored:

```
add_data_files
add_data_dir
finalize_options
CCompiler:customize
get_paths
get_atlas_version
setup
_exec_command
_scalar_str
long_double_representaion
analyzeline
analyzevars
```

## Effort spent


In average the time spent for each team member:


1. plenary discussions/meetings; 5-6 hours


2. discussions within parts of the group; 
This is a hard estimation, but we were regularly in contact with each other using slack. 


3. reading documentation; 1-2 hours


4. configuration; around 1 hour.


5. analyzing code/output; 2-3 hours


6. writing documentation; 2 hour


7. writing code; 20 hours (in average)


8. running code? 1 hour 
(we had a large test base and all the tests took around 1 min to run.)


## Overall experience


Using coverage tools. Don’t create too many branches. Improved python skills.